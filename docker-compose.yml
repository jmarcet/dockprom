version: '2.4'

services:

  prometheus:
    image: prom/prometheus:v2.24.0
    container_name: prometheus
    volumes:
      - ./prometheus:/etc/prometheus
      - /opt/monitoring/prometheus:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    restart: on-failure
    expose:
      - 9090
    mac_address: ${MAC_PROMETHEUS}
    extra_hosts:
      collectd: ${LAN_IP}
      nodeexporter: ${LAN_IP}
    ports:
      - '127.0.0.1:9090:9090'
    labels:
      org.label-schema.group: "monitoring"
    cpu_shares: 256
    cpus: 1
    mem_limit: 512mb
    memswap_limit: 512mb
    healthcheck:
      test: wget -S http://127.0.0.1:9090/ 2>&1 | grep -q 'HTTP/1.1 302 Found'
      interval: 30s
      timeout: 5s
      retries: 3

  #alertmanager:
  #  image: prom/alertmanager:v0.21.0
  #  container_name: alertmanager
  #  volumes:
  #    - ./alertmanager:/etc/alertmanager
  #  command:
  #    - '--config.file=/etc/alertmanager/config.yml'
  #    - '--storage.path=/alertmanager'
  #  restart: unless-stopped
  #  expose:
  #    - 9093
  #  networks:
  #    - monitor-net
  #  labels:
  #    org.label-schema.group: "monitoring"

  nodeexporter:
    image: prom/node-exporter:v1.0.1
    container_name: nodeexporter
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.diskstats.ignored-devices=^(loop|dm-|nbd|nvme\dn\dp|sd[a-z])\d+'
      - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc|opt/.+)($$|/)'
      - '--collector.netclass.ignored-devices=^(bond0|dummy0|eth[01]|eth0\.6|ifb4.+|ip6tnl0|lo|teql0|veth.+)$$'
      - '--collector.netdev.device-blacklist=^(bond0|dummy0|eth[01]|eth0\.6|ifb4.+|ip6tnl0|lo|teql0|veth.+)$$'
      - '--collector.interrupts'
      - '--collector.processes'
      - '--collector.tcpstat'
      - '--web.listen-address=${LAN_IP}:9100'
    restart: on-failure
    network_mode: host
    labels:
      org.label-schema.group: "monitoring"
    cpu_shares: 256
    cpus: 1
    mem_limit: 64mb
    memswap_limit: 64mb
    healthcheck:
      test: wget -S http://${LAN_IP}:9100/ 2>&1 | grep -q 'HTTP/1.1 200 OK'
      interval: 30s
      timeout: 5s
      retries: 3

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.38.6
    container_name: cadvisor
    volumes:
      - /:/rootfs:ro
      - /tmp/run:/var/run:rw
      - /tmp/run/docker/containerd/containerd.sock:/run/containerd/containerd.sock:rw
      - /sys:/sys:ro
      - /opt/docker:/var/lib/docker:ro
    restart: on-failure
    expose:
      - 8080
    mac_address: ${MAC_CADVISOR}
    ports:
      - '127.0.0.1:8080:8080'
    labels:
      org.label-schema.group: "monitoring"
    cpu_shares: 256
    cpus: 1
    mem_limit: 256mb
    memswap_limit: 256mb
    healthcheck:
      test: wget -S http://127.0.0.1:8080/ 2>&1 | grep -q 'HTTP/1.1 307 Temporary Redirect'
      interval: 30s
      timeout: 5s
      retries: 3

  grafana:
    image: grafana/grafana:7.3.6
    container_name: grafana
    volumes:
      - /opt/monitoring/grafana:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    environment:
      - GF_SECURITY_ADMIN_USER=${ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${ADMIN_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-clock-panel,natel-discrete-panel,grafana-piechart-panel
    restart: on-failure
    expose:
      - 3000
    mac_address: ${MAC_GRAFANA}
    ports:
      - '127.0.0.1:3000:3000'
    labels:
      org.label-schema.group: "monitoring"
    cpu_shares: 256
    cpus: 1
    mem_limit: 256mb
    memswap_limit: 256mb
    healthcheck:
      test: wget -S http://127.0.0.1:3000/ 2>&1 | grep -q 'HTTP/1.1 302 Found'
      interval: 30s
      timeout: 5s
      retries: 3

  #pushgateway:
  #  image: prom/pushgateway:v1.3.1
  #  container_name: pushgateway
  #  restart: on-failure
  #  expose:
  #    - 9091
  #  labels:
  #    org.label-schema.group: "monitoring"

  unifi:
    image: linuxserver/unifi-controller:latest
    container_name: unifi
    restart: on-failure
    mac_address: ${MAC_UNIFI}
    ports:
      - '${LAN_IP}:3478:3478/udp'
      - '${LAN_IP}:10001:10001/udp'
      - '${LAN_IP}:6789:6789'
      - '${LAN_IP}:8080:8080'
      - '${LAN_IP}:8443:8443'
      - '${LAN_IP}:8880:8880'
    volumes:
      - /etc/localtime:/etc/localtime
      - /etc/acme:/etc/acme
      - /opt/unifi:/config
    environment:
      - PUID=${PUID}
      - PGID=${PGID}
    cpu_shares: 256
    cpus: 1
    mem_limit: 1gb
    memswap_limit: 1gb
    healthcheck:
      test: curl --fail -k -s https://127.0.0.1:8443/
      interval: 30s
      timeout: 5s
      retries: 3

  unifipoller:
    image: golift/unifi-poller:latest
    container_name: unifipoller
    restart: on-failure
    mac_address: ${MAC_UNIFI_POLLER}
    labels:
      org.label-schema.group: "monitoring"
    environment:
      - UP_INFLUXDB_DISABLE=true
      - UP_POLLER_DEBUG=${UP_POLLER_DEBUG}
      - UP_UNIFI_DEFAULT_USER=${UP_UNIFI_USER}
      - UP_UNIFI_DEFAULT_PASS=${UP_UNIFI_PASS}
      - UP_UNIFI_DEFAULT_URL=https://unifi:8443/
      - UP_UNIFI_DYNAMIC=false
    cpu_shares: 256
    cpus: 1
    mem_limit: 64mb
    memswap_limit: 64mb

networks:
  default:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: ${SUBNET_MONITORING}
    driver_opts:
      com.docker.network.bridge.name: br-monitoring

